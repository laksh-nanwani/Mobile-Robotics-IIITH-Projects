{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mexican-confirmation",
   "metadata": {},
   "source": [
    "# Question 4: General Theory/Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-cleaners",
   "metadata": {},
   "source": [
    "_No need to be verbose, it's not fun for anyone_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-hindu",
   "metadata": {},
   "source": [
    "### 1. What part of S**L**A**M** did this project deal with? Why? What does the other part deal with and how would it generally work, given that you only have LIDAR scans, RGB video stream, and noisy pose data for a moving robot?\n",
    "\n",
    "We dealt with localisation in this project. We dealt with the estimate of the robot's pose with respect to some map and dealing with the noise in those poses.\n",
    "The other part is mapping. It deals with the collection and connecting different sensor measurements into a common map with the help of LIDAR scans, RGB video Stream, noisy pose data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5010a3",
   "metadata": {},
   "source": [
    "### 2. Loop closures play an important role in reducing drift, how would you go about detecting these?\n",
    "\n",
    "Loop closure can be detected using **visual place recognition**. In this technique entire database of images is parsed to find the best match for the current image. The essential condition for this algorithm is that the current image is actually a revisited place and a reference image is present in our database.\n",
    "\n",
    "<img src=\"../data/loop_closure.png\" alt=\"Drawing\"/>\n",
    "\n",
    "Let's consider a case of a robot moving in an indoor space, as shown in the image. Here when we look at pose number 8, we have more constraints than usual. Now for each pose least square error of the global graph is minimized, to reduce further the effect of outliers, robust cost functions (like Huber loss) are used.\n",
    "\n",
    "Once we are sure about the loop closure, it is likely that our current image/pose remains unadjusted and rather adjust the poses of all non-active frame poses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a4c69",
   "metadata": {},
   "source": [
    "### 3. Explain the structure of your Jacobian. Is the pose-graph fully connected? Why/Why not?\n",
    "\n",
    "The jacobian of residual(k,j) w.r.t. a pose k is ,\n",
    "                            \n",
    "$$ \\mathbf{J_{s(k,j)}} =  \\left[\\begin{array}{ccc}\n",
    "1 & 0 & -\\Delta x_{(k,j)} \\sin(\\theta_k) - \\Delta y_{(k,j)} \\cos(\\theta_k) \\\\\n",
    "0 & 1 & -\\Delta y_{(k,j)} \\sin(\\theta_k) + \\Delta x_{(k,j)} \\cos(\\theta_k) \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{array}\\right]$$\n",
    "</br>\n",
    "Also the jacobian of the same residual w.r.t. pose j is,\n",
    "$$ \\mathbf{J_{n(k,j)}} =  \\left[\\begin{array}{ccc}\n",
    "-1 & 0 & 0 \\\\\n",
    "0 & -1 & 0 \\\\\n",
    "0 & 0 & -1 \\\\\n",
    "\\end{array}\\right]$$\n",
    "</br>\n",
    "The Jocabian is given by,\n",
    "<img src=\"../data/jacobian_structure.png\" alt=\"Drawing\"/>\n",
    "The Shape of the jacobian is (m+n+1) * N x (m+1) * N,</br>\n",
    "where m is no. of poses(vertices), 1 prior , hence (m+1) poses</br>\n",
    "n is no. of loop closure constraints, N is no. of variables. \n",
    "\n",
    "Shape in our case is 420 by 360.\n",
    "\n",
    "The Jacobian matrix is sparse in nature, only filled for 2 * N entries per row (except last N rows, which represent prior). We can also observe that the columns filled represent the edges between poses (both normal and loop closure).\n",
    "\n",
    "The shape is kind off diagonal for the part of matrix which represents adjacent edges.\n",
    "</br>\n",
    "</br>\n",
    "</br>\n",
    "**Is the pose-graph fully connected? Why/Why not?**\n",
    "\n",
    "No, the graph is not fully connected. We get a sparse one as the sensor range is limited to the robot specifications.\n",
    "\n",
    "<img src=\"../data/sparse.png\" alt=\"Drawing\"/>\n",
    "\n",
    "Source: Factor Graphs for Robot Perception, Daellart and Kaess, Figure 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0165b1",
   "metadata": {},
   "source": [
    "### 4. With what you know now, how would you describe and differentiate the SLAM frontend and backend? Why do we need to optimise our poses/map in the first place - where does the noise come from/why?\n",
    "Front-end is to get the raw data from the sensors (laser-based, RGBD, etc.) and convert that to a graph. It is used mainly for **GRAPH CONSTRUCTION**. \n",
    "The main task includes -\n",
    "1. Odometry information:- New vertex in a graph is added according to the odometry constraints\n",
    "2. Handling loop closures:- Analysing the whole trajectory, the front end should detect any place which has been visited multiple times and generate constraints accordingly.\n",
    "3. Removing outliers\n",
    "\n",
    "The back end is used to optimize the initial graph proposed by the front end using odometry information. It is used primarily for **GRAPH OPTIMISATION**. The main task is to \n",
    "1. Find the Maximum A-posteriori estimate that explains all the observation\n",
    "2. Reduce the least square error\n",
    "\n",
    "If we get perfect odometry information, then there is no need of SLAM for optimization. But we know there is always some noise present in the odometry information, These algorithms work well in a local environment, but on a global scale, these drift in odometry will lead to significant errors; therefore optimization of poses/maps using SLAM is required.\n",
    "\n",
    "Generally, this odometry data is extracted from the visual information, i.e. difference of the current image to the previous image, which gives us the incremental change in the pose, which ultimately leads to the drift after several readings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
